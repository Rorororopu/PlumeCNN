{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import analyzer\n",
    "import neural_network as nn\n",
    "import convolutional_neural_network as cnn\n",
    "import visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = [250,250]\n",
    "# Resolution should be consistent throughout the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Because the training dataset are individual, the list of each data just contain themselves.\n",
    "data1 = analyzer.Data(\"MORE_DATA/db_Y_0027.okc\", resolution) \n",
    "data2 = analyzer.Data(\"MORE_DATA/db_Y_0030.okc\", resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use NN model\n",
    "\n",
    "array1, headers1, non_nan_indices1, num_grids1 = nn.data_arranger(data1.df)\n",
    "array2, headers2, non_nan_indices2, num_grids2 = nn.data_arranger(data2.df)\n",
    "\n",
    "# The learning rate, batch size, and epochs are proven to be working.\n",
    "\n",
    "nn_model = nn.model_create_compile(headers1, 0.05)\n",
    "\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array1, 1000, 10)\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array2, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use CNN model\n",
    "\n",
    "array1, headers1, indices1 = cnn.data_arranger(data1.df, resolution)\n",
    "array2, headers2, indices2 = cnn.data_arranger(data2.df, resolution)\n",
    "\n",
    "# The learning rate and epochs are proven to be working.\n",
    "\n",
    "cnn_model = cnn.model_2D_create_compile(headers1, 0.05, resolution)\n",
    "\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array1, 3)\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = analyzer.Data(\"MORE_DATA/db_Y_0049.okc\", resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "array3, headers3, non_nan_indices3, num_grids3 = nn.data_arranger(data3.df)\n",
    "data3.df = nn.model_classification(nn_model, array3, non_nan_indices3, num_grids3, data3.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "array3, headers3, indices3 = cnn.data_arranger(data3.df, resolution)\n",
    "data3.df = cnn.model_2D_classification(cnn_model, array3, indices3, data3.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_classify = [f\"MORE_DATA/db_Y_{i:04d}.okc\" for i in range(99)]\n",
    "\n",
    "Data_classify = [analyzer.Data(path, list_paths_classify, resolution) for path in list_paths_classify] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "for data in Data_classify:\n",
    "    array, headers, non_nan_indices, num_grids = nn.data_arranger(data.df)\n",
    "    data.df = nn.model_classification(nn_model, array, non_nan_indices, num_grids, data.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "for data in Data_classify:\n",
    "    array, headers, indices = cnn.data_arranger(data.df, data.resolution)\n",
    "    data.df = cnn.model_2D_classification(cnn_model, array, indices, data.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "visualizer.plot_2D_df(data3.df, 'is_boundary', 'classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "data3.df.to_csv('classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 3D CSVs\n",
    "\n",
    "will remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "path = '3D_DATA/small_db_result'\n",
    "output_file = '3D_DATA/small_db.csv'\n",
    "\n",
    "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = None\n",
    "    \n",
    "    for index, filename in enumerate(files):\n",
    "        print(f\"merging file: {index + 1}/{len(files)}\", flush=True)\n",
    "        \n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            header = next(reader)\n",
    "            \n",
    "            if writer is None:\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"All files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import plot_3D_to_2D_slice_streamline\n",
    "\n",
    "plot_3D_to_2D_slice_streamline(input_file=\"3D_DATA/ra10e7_result/ra10e7_100.csv\", output_file=\"streamlines.html\", direction='y', seed_points_resolution=[20,20], max_time=0.2, cmap = 'viridis', axis_limits=[-0.5,0.5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ from a manually made velocity field, plot streamline (for test)\n",
    "\n",
    "✅ generate a csv file from a set velocity field(for test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n",
    "\n",
    "# Extract unique coordinate values for each axis (ensure they are sorted)\n",
    "x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())\n",
    "\n",
    "# Determine grid dimensions\n",
    "nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the StructuredGrid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Add the velocity vectors\n",
    "grid.point_data['velocity'] = velocities\n",
    "\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(-0.5, 0.5, 3), # min, max, num\n",
    "    np.linspace(-0.5, 0.5, 3),\n",
    "    np.linspace(-0.5, 0.5, 3)\n",
    "    )\n",
    "seed_x = seed_x.ravel()\n",
    "seed_y = seed_y.ravel()\n",
    "seed_z = seed_z.ravel()\n",
    "\n",
    "seed_points = np.column_stack((seed_x, seed_y, seed_z))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")\n",
    "\n",
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.01),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dealing with empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_empty_rows(file_path:str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fills in the empty rows in a CSV file that represents a Y-slice 2D data grid. \n",
    "    The data is structured in repeating \"sandwich\" blocks where each block contains:\n",
    "        [empty rows] + [non-empty data rows] + [empty rows].\n",
    "    The missing rows (empty) occur due to a truncated `x` range. Non-empty data rows have same z coordinate\n",
    "    but different x coordinates. The dataset is already on a regular grid, so no interpolation is needed. \n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the input CSV file containing possibly incomplete 2D slice data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            A DataFrame with the same shape as the original but with all empty rows filled in. \n",
    "            Only includes the essential columns: \n",
    "            ['x', 'y', 'z', 'x_velocity', 'y_velocity', 'z_velocity'].\n",
    "    '''\n",
    "\n",
    "    # Step 1: read df(dataframe)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    required_columns = ['x', 'y', 'z', 'x_velocity', 'y_velocity', 'z_velocity']\n",
    "    df = df[required_columns]\n",
    "\n",
    "    df_filled = df.copy()\n",
    "    empty_mask = df.isna().all(axis=1) # A list length is num of rows. If all row is empty, corresponding element is True.\n",
    "\n",
    "    # Step 2: Count initial empty rows\n",
    "    first_non_empty_idx = empty_mask.idxmin()\n",
    "    num_empty_rows = empty_mask[:first_non_empty_idx].sum()\n",
    "\n",
    "    # Step 3: Count non-empty rows in the first pack\n",
    "    idx = first_non_empty_idx\n",
    "    while not empty_mask.iloc[idx]: # iloc[idx]: if that index is True, then True\n",
    "        idx += 1\n",
    "    num_non_empty_rows = idx - first_non_empty_idx\n",
    "\n",
    "    # Step 5: Get x_diff from first block of non-empty rows\n",
    "    x_vals = df_filled.loc[first_non_empty_idx:idx - 1, 'x'].values #loc: range of row, header, values() is return np array\n",
    "    x_diff = np.diff(x_vals).mean()\n",
    "\n",
    "    # Step 6: Precompute total number of \"sandwiches\"\n",
    "    group_size = num_empty_rows * 2 + num_non_empty_rows\n",
    "    total_groups = len(df_filled) // group_size\n",
    "\n",
    "    for i in range(total_groups):\n",
    "        # Calculate indices for each part of the sandwich\n",
    "        start_idx = i * group_size\n",
    "        mid_idx = start_idx + num_empty_rows\n",
    "        end_idx = mid_idx + num_non_empty_rows\n",
    "\n",
    "        # First and last x values of the current non-empty block\n",
    "        block = df_filled.iloc[mid_idx:end_idx]\n",
    "        if block['x'].isna().all():\n",
    "            continue  # This block has no valid data, skip\n",
    "\n",
    "        x_first = block['x'].iloc[0]\n",
    "        x_last = block['x'].iloc[-1]\n",
    "\n",
    "        y_first = block['y'].iloc[0]\n",
    "        z_first = block['z'].iloc[0]\n",
    "        y_last = block['y'].iloc[-1]\n",
    "        z_last = block['z'].iloc[-1]\n",
    "\n",
    "        # Build x values for rows ABOVE the block\n",
    "        x_above = x_first - np.arange(num_empty_rows, 0, -1) * x_diff\n",
    "        rows_above = pd.DataFrame({\n",
    "            'x': x_above,\n",
    "            'y': y_first,\n",
    "            'z': z_first,\n",
    "            'x_velocity': 0.0,\n",
    "            'y_velocity': 0.0,\n",
    "            'z_velocity': 0.0\n",
    "        })\n",
    "\n",
    "        # Build x values for rows BELOW the block\n",
    "        x_below = x_last + np.arange(1, num_empty_rows + 1) * x_diff\n",
    "        rows_below = pd.DataFrame({\n",
    "            'x': x_below,\n",
    "            'y': y_last,\n",
    "            'z': z_last,\n",
    "            'x_velocity': 0.0,\n",
    "            'y_velocity': 0.0,\n",
    "            'z_velocity': 0.0\n",
    "        })\n",
    "\n",
    "        # Insert filled data into the copied DataFrame\n",
    "        df_filled.iloc[start_idx:mid_idx] = rows_above.values\n",
    "        df_filled.iloc[end_idx:end_idx + num_empty_rows] = rows_below.values\n",
    "\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import glob\n",
    "import os\n",
    "# in such/a/folder/name_result, the CSVs will be like name_1.csv, name_2.csv, ...\n",
    "\n",
    "# Read and concatenate all CSV files in the folder\n",
    "folder_path = '3D_DATA/ra10e7_result'\n",
    "# Extract the last part of the path and remove 'result'\n",
    "base_name = os.path.basename(folder_path).replace('result', '')\n",
    "# Create the search pattern\n",
    "pattern = f\"{base_name}*.csv\"\n",
    "# Get sorted list of matching CSV files\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, pattern)))\n",
    "df_list = [fill_empty_rows(f) for f in csv_files]\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x_velocity</th>\n",
       "      <th>y_velocity</th>\n",
       "      <th>z_velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9204</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9206</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9207</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11002</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11003</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11005</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17603</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17607</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26603</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26607</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x   y   z  x_velocity  y_velocity  z_velocity\n",
       "9200  NaN NaN NaN         0.0         0.0         0.0\n",
       "9201  NaN NaN NaN         0.0         0.0         0.0\n",
       "9202  NaN NaN NaN         0.0         0.0         0.0\n",
       "9203  NaN NaN NaN         0.0         0.0         0.0\n",
       "9204  NaN NaN NaN         0.0         0.0         0.0\n",
       "9205  NaN NaN NaN         0.0         0.0         0.0\n",
       "9206  NaN NaN NaN         0.0         0.0         0.0\n",
       "9207  NaN NaN NaN         NaN         NaN         NaN\n",
       "11000 NaN NaN NaN         0.0         0.0         0.0\n",
       "11001 NaN NaN NaN         0.0         0.0         0.0\n",
       "11002 NaN NaN NaN         0.0         0.0         0.0\n",
       "11003 NaN NaN NaN         0.0         0.0         0.0\n",
       "11004 NaN NaN NaN         0.0         0.0         0.0\n",
       "11005 NaN NaN NaN         0.0         0.0         0.0\n",
       "11006 NaN NaN NaN         0.0         0.0         0.0\n",
       "11007 NaN NaN NaN         NaN         NaN         NaN\n",
       "17600 NaN NaN NaN         0.0         0.0         0.0\n",
       "17601 NaN NaN NaN         0.0         0.0         0.0\n",
       "17602 NaN NaN NaN         0.0         0.0         0.0\n",
       "17603 NaN NaN NaN         0.0         0.0         0.0\n",
       "17604 NaN NaN NaN         0.0         0.0         0.0\n",
       "17605 NaN NaN NaN         0.0         0.0         0.0\n",
       "17606 NaN NaN NaN         0.0         0.0         0.0\n",
       "17607 NaN NaN NaN         NaN         NaN         NaN\n",
       "26600 NaN NaN NaN         0.0         0.0         0.0\n",
       "26601 NaN NaN NaN         0.0         0.0         0.0\n",
       "26602 NaN NaN NaN         0.0         0.0         0.0\n",
       "26603 NaN NaN NaN         0.0         0.0         0.0\n",
       "26604 NaN NaN NaN         0.0         0.0         0.0\n",
       "26605 NaN NaN NaN         0.0         0.0         0.0\n",
       "26606 NaN NaN NaN         0.0         0.0         0.0\n",
       "26607 NaN NaN NaN         NaN         NaN         NaN"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isna().any(axis=1)] # if any nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n",
    "\n",
    "# Extract unique coordinate values for each axis (ensure they are sorted)\n",
    "x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())\n",
    "\n",
    "# Determine grid dimensions\n",
    "nx = 200\n",
    "ny = 200\n",
    "nz = 198\n",
    "# nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the StructuredGrid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Add the velocity vectors\n",
    "grid.point_data['velocity'] = velocities\n",
    "\n",
    "xmin, xmax, ymin, ymax, zmin, zmax = grid.bounds\n",
    "\n",
    "# Create a 5×5×5 grid of seed points across the domain\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(xmin, xmax, 25),\n",
    "    np.linspace(ymin, ymax, 25),\n",
    "    np.linspace(zmin, zmax, 25),\n",
    "    indexing='ij'\n",
    ")\n",
    "\n",
    "seed_points = np.column_stack((\n",
    "    seed_x.ravel(), seed_y.ravel(), seed_z.ravel()\n",
    "))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='width: 100%;'><tr><th>Header</th><th>Data Arrays</th></tr><tr><td>\n",
       "<table style='width: 100%;'>\n",
       "<tr><th>PolyData</th><th>Information</th></tr>\n",
       "<tr><td>N Cells</td><td>145</td></tr>\n",
       "<tr><td>N Points</td><td>5548</td></tr>\n",
       "<tr><td>N Strips</td><td>0</td></tr>\n",
       "<tr><td>X Bounds</td><td>-5.000e-01, 4.899e-01</td></tr>\n",
       "<tr><td>Y Bounds</td><td>-4.897e-01, 4.950e-01</td></tr>\n",
       "<tr><td>Z Bounds</td><td>5.947e-03, 9.942e-01</td></tr>\n",
       "<tr><td>N Arrays</td><td>8</td></tr>\n",
       "</table>\n",
       "\n",
       "</td><td>\n",
       "<table style='width: 100%;'>\n",
       "<tr><th>Name</th><th>Field</th><th>Type</th><th>N Comp</th><th>Min</th><th>Max</th></tr>\n",
       "<tr><td><b>velocity</b></td><td>Points</td><td>float64</td><td>3</td><td>-4.590e-01</td><td>3.286e-01</td></tr>\n",
       "<tr><td>IntegrationTime</td><td>Points</td><td>float64</td><td>1</td><td>-1.764e+01</td><td>2.076e+01</td></tr>\n",
       "<tr><td>Vorticity</td><td>Points</td><td>float64</td><td>3</td><td>-4.273e+02</td><td>4.282e+02</td></tr>\n",
       "<tr><td>Rotation</td><td>Points</td><td>float64</td><td>1</td><td>-5.317e+02</td><td>6.332e+02</td></tr>\n",
       "<tr><td>AngularVelocity</td><td>Points</td><td>float64</td><td>1</td><td>-3.208e+02</td><td>2.958e+02</td></tr>\n",
       "<tr><td>Normals</td><td>Points</td><td>float64</td><td>3</td><td>-9.998e-01</td><td>1.000e+00</td></tr>\n",
       "<tr><td>ReasonForTermination</td><td>Cells</td><td>int32</td><td>1</td><td>1.000e+00</td><td>6.000e+00</td></tr>\n",
       "<tr><td>SeedIds</td><td>Cells</td><td>int32</td><td>1</td><td>1.000e+00</td><td>9.900e+01</td></tr>\n",
       "</table>\n",
       "\n",
       "</td></tr> </table>"
      ],
      "text/plain": [
       "PolyData (0x7fae2811d360)\n",
       "  N Cells:    145\n",
       "  N Points:   5548\n",
       "  N Strips:   0\n",
       "  X Bounds:   -5.000e-01, 4.899e-01\n",
       "  Y Bounds:   -4.897e-01, 4.950e-01\n",
       "  Z Bounds:   5.947e-03, 9.942e-01\n",
       "  N Arrays:   8"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.005),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
