{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import analyzer\n",
    "import neural_network as nn\n",
    "import convolutional_neural_network as cnn\n",
    "import visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = [250,250]\n",
    "# Resolution should be consistent throughout the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Because the training dataset are individual, the list of each data just contain themselves.\n",
    "data1 = analyzer.Data(\"MORE_DATA/db_Y_0027.okc\", resolution) \n",
    "data2 = analyzer.Data(\"MORE_DATA/db_Y_0030.okc\", resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use NN model\n",
    "\n",
    "array1, headers1, non_nan_indices1, num_grids1 = nn.data_arranger(data1.df)\n",
    "array2, headers2, non_nan_indices2, num_grids2 = nn.data_arranger(data2.df)\n",
    "\n",
    "# The learning rate, batch size, and epochs are proven to be working.\n",
    "\n",
    "nn_model = nn.model_create_compile(headers1, 0.05)\n",
    "\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array1, 1000, 10)\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array2, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use CNN model\n",
    "\n",
    "array1, headers1, indices1 = cnn.data_arranger(data1.df, resolution)\n",
    "array2, headers2, indices2 = cnn.data_arranger(data2.df, resolution)\n",
    "\n",
    "# The learning rate and epochs are proven to be working.\n",
    "\n",
    "cnn_model = cnn.model_2D_create_compile(headers1, 0.05, resolution)\n",
    "\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array1, 3)\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = analyzer.Data(\"MORE_DATA/db_Y_0049.okc\", resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "array3, headers3, non_nan_indices3, num_grids3 = nn.data_arranger(data3.df)\n",
    "data3.df = nn.model_classification(nn_model, array3, non_nan_indices3, num_grids3, data3.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "array3, headers3, indices3 = cnn.data_arranger(data3.df, resolution)\n",
    "data3.df = cnn.model_2D_classification(cnn_model, array3, indices3, data3.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_classify = [f\"MORE_DATA/db_Y_{i:04d}.okc\" for i in range(99)]\n",
    "\n",
    "Data_classify = [analyzer.Data(path, list_paths_classify, resolution) for path in list_paths_classify] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "for data in Data_classify:\n",
    "    array, headers, non_nan_indices, num_grids = nn.data_arranger(data.df)\n",
    "    data.df = nn.model_classification(nn_model, array, non_nan_indices, num_grids, data.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "for data in Data_classify:\n",
    "    array, headers, indices = cnn.data_arranger(data.df, data.resolution)\n",
    "    data.df = cnn.model_2D_classification(cnn_model, array, indices, data.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "visualizer.plot_2D_df(data3.df, 'is_boundary', 'classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "data3.df.to_csv('classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "for i, data in enumerate(Data_classify):\n",
    "    visualizer.plot_2D_df(data.df, 'is_boundary', f'classification_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "for i, data in enumerate(Data_classify):\n",
    "    data.df.to_csv(f'classification_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "path = '3D_DATA/small_db_result'\n",
    "output_file = '3D_DATA/small_db.csv'\n",
    "\n",
    "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = None\n",
    "    \n",
    "    for index, filename in enumerate(files):\n",
    "        print(f\"merging file: {index + 1}/{len(files)}\", flush=True)\n",
    "        \n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            header = next(reader)\n",
    "            \n",
    "            if writer is None:\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"All files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import plot_3D_to_2D_slice_streamline\n",
    "\n",
    "plot_3D_to_2D_slice_streamline(input_file=\"3D_DATA/ra10e7_result/ra10e7_100.csv\", output_file=\"streamlines.html\", direction='y', seed_points_resolution=[20,20], max_time=0.2, cmap = 'viridis', axis_limits=[-0.5,0.5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import scipy\n",
    "import numpy as np\n",
    "import vtk\n",
    "import pandas as pd\n",
    "\n",
    "def arrange_slice_df(input_file: str, direction:str, output_df: str = None, cubic: bool = True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Arrange sliced data to prepare for plotting streamlines.\n",
    "    Fill empty rows with coordinates to ensure a complete grid of points in a square.\n",
    "    For NaN points outside the container, set velocity to 0.\n",
    "    For NaN points inside the container, interpolate velocity as needed.\n",
    "\n",
    "    Note that because even data for 3D are stored in many slices, so it is necessary to specify a slicing direction.\n",
    "    Args:\n",
    "        input_file: The path of the SLICED CSV file (preprocessed by our PlumeCNN).\n",
    "        Direction: Specify the axis to which the slice is perpendicular ('x', 'y', or 'z'). \n",
    "        output_file: If you want to output the arranged data to CSV, please input the desired path of the output file.\n",
    "            If None(in default), it will not write any file out.\n",
    "        if_cubic: If True (default), the function will process coordinates and velocity components in all directions.\n",
    "            If false, Only coordinates and velocity components in the remaining directions will be considered. \n",
    "            The reason why this variable is called 'cubic' is because only a slice representing for a whole cubic data needs this option.\n",
    "           \n",
    "    Returns:The arranged pandas dataframe, containing coordinate values and velocities in desired directions.\n",
    "    '''\n",
    "    df = pd.read_csv(input_file)\n",
    "    # Read and extract columns to read\n",
    "    if direction == 'z':\n",
    "        coord1 = 'x'\n",
    "        coord2 = 'y'\n",
    "        coord3 = 'z'\n",
    "        vel1 = 'x_velocity'\n",
    "        vel2 = 'y_velocity'\n",
    "        vel3 = 'z_velocity'\n",
    "    elif direction == 'y':\n",
    "        coord1 = 'x'\n",
    "        coord2 = 'z'\n",
    "        coord3 = 'y'\n",
    "        vel1 = 'x_velocity'\n",
    "        vel2 = 'z_velocity'\n",
    "        vel3 = 'y_velocity'\n",
    "    elif direction == 'x':\n",
    "        coord1 = 'y'\n",
    "        coord2 = 'z'\n",
    "        coord3 = 'x'\n",
    "        vel1 = 'y_velocity'\n",
    "        vel2 = 'z_velocity'\n",
    "        vel3 = 'x_velocity'\n",
    "\n",
    "    if cubic is True:\n",
    "        df = df[['x', 'y', 'z', 'x_velocity', 'y_velocity', 'z_velocity']].dropna()\n",
    "    else:\n",
    "        df = df[[coord1, coord2, vel1, vel2]].dropna()\n",
    "\n",
    "    # Extract unique coordinate values, sorted in ascending order\n",
    "    coord1_values = np.sort(df[coord1].unique())\n",
    "    coord2_values = np.sort(df[coord2].unique())\n",
    "    # define grid points in the specified plane for interpolation\n",
    "    grid_coord1, grid_coord2 = np.meshgrid(coord1_values, coord2_values, indexing='ij')\n",
    "    # Prepare data points and velocity components for interpolation\n",
    "    points = df[[coord1, coord2]].values\n",
    "    u_values = df[vel1].values\n",
    "    v_values = df[vel2].values\n",
    "    if cubic is True:\n",
    "        w_values = df[vel3].values\n",
    "    # Interpolate velocity components onto the grid, to eliminate NaN points lying in the dataset\n",
    "    '''without this step, NaN points in the grid will make the streamline extremely short'''\n",
    "    u_grid = scipy.interpolate.griddata(points, u_values, (grid_coord1, grid_coord2), method='linear')\n",
    "    v_grid = scipy.interpolate.griddata(points, v_values, (grid_coord1, grid_coord2), method='linear')\n",
    "    if cubic is True:\n",
    "        w_grid = scipy.interpolate.griddata(points, w_values, (grid_coord1, grid_coord2), method='linear')\n",
    "    # Replace any NaN values in interpolated data with zeros, to specify the border of the valid data\n",
    "    u_grid = np.nan_to_num(u_grid, nan=0.0).ravel(order='F')\n",
    "    v_grid = np.nan_to_num(v_grid, nan=0.0).ravel(order='F')\n",
    "    if cubic is True:\n",
    "        w_grid = np.nan_to_num(w_grid, nan=0.0).ravel(order='F')\n",
    "    \n",
    "    # Prepare the final DataFrame for return\n",
    "    if direction == 'z':\n",
    "        result_df = pd.DataFrame({\n",
    "            'x': grid_coord1.ravel(order='F'),\n",
    "            'y': grid_coord2.ravel(order='F'),\n",
    "            'x_velocity': u_grid,\n",
    "            'y_velocity': v_grid\n",
    "        })\n",
    "        if cubic is True:\n",
    "            result_df['z'] = df[coord3].unique().mean()\n",
    "            result_df['z_velocity'] = w_grid\n",
    "    elif direction == 'y':\n",
    "        result_df = pd.DataFrame({\n",
    "            'x': grid_coord1.ravel(order='F'),\n",
    "            'z': grid_coord2.ravel(order='F'),\n",
    "            'x_velocity': u_grid,\n",
    "            'z_velocity': v_grid\n",
    "        })\n",
    "        if cubic is True:\n",
    "            result_df['y'] = df[coord3].unique().mean()\n",
    "            result_df['y_velocity'] = w_grid\n",
    "    elif direction == 'x':\n",
    "        result_df = pd.DataFrame({\n",
    "            'y': grid_coord1.ravel(order='F'),\n",
    "            'z': grid_coord2.ravel(order='F'),\n",
    "            'y_velocity': u_grid,\n",
    "            'z_velocity': v_grid\n",
    "        })\n",
    "        if cubic is True:\n",
    "            result_df['x'] = df[coord3].unique().mean()\n",
    "            result_df['x_velocity'] = w_grid\n",
    "    # Save to CSV if output path is provided\n",
    "    if output_df is not None:\n",
    "        result_df.to_csv(output_df, index=False)\n",
    "\n",
    "    # Return the result DataFrame\n",
    "    return result_df\n",
    "\n",
    "def plot_3D_to_2D_slice_streamline(input_file: str, output_file: str, direction: str, seed_points_resolution: list, integration_direction: str = 'both', max_time: float = 0.2, terminal_speed: float = 1e-5, cmap: str = 'viridis', axis_limits: list = None):\n",
    "    '''\n",
    "    Generate and visualize 2D streamlines from a 3D dataset, projected onto a specified plane,\n",
    "    and export the visualization as an HTML file. The seed points (starting points) of the streamlines are evenly distributed, with the resolution specified by the user.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to the CSV file containing the 3D dataset.\n",
    "        output_file: Path to the output HTML file for the visualization, sontaining the extension.\n",
    "        direction: The direction to which the plane is perpendicular ('x', 'y' or 'z').\n",
    "        seed_points_resolution: Specifies the resolution for distributing seed points in the format [var1_resolution, var2_resolution], where each element controls the density along the respective variable axis.\n",
    "        integration_direction (optional): Specify whether the streamline is integrated in the upstream or downstream directions (or both). Options are 'both'(default), 'backward', or 'forward'.\n",
    "        max_time (optional): What is the maximum integration time of a streamline (0.2 in default).\n",
    "        terminal_speed (optional): When will the integration stop (1e-5 in default).\n",
    "        cmap (optional): Colormap to use for the visualization. Default is 'viridis'.\n",
    "        axis_limits (optional): A list of axis limits for the plot in the form [var1_min, var1_max, var2_min, var2_max].\n",
    "            If None, the axis limits will be determined automatically from the data.\n",
    "    '''\n",
    "\n",
    "    # Suppress all VTK warnings and errors\n",
    "    '''\n",
    "    If not, we will get the warning \"Unable to factor linear system\" for every streamline we plot, although the result is quite good.\n",
    "    '''\n",
    "    vtk.vtkObject.GlobalWarningDisplayOff()\n",
    "\n",
    "    # Define coordinate and velocity components based on the specified direction\n",
    "    if direction == 'z':\n",
    "        coord1 = 'x'\n",
    "        coord2 = 'y'\n",
    "        vel1 = 'x_velocity'\n",
    "        vel2 = 'y_velocity'\n",
    "    elif direction == 'y':\n",
    "        coord1 = 'x'\n",
    "        coord2 = 'z'\n",
    "        vel1 = 'x_velocity'\n",
    "        vel2 = 'z_velocity'\n",
    "    elif direction == 'x':\n",
    "        coord1 = 'y'\n",
    "        coord2 = 'z'\n",
    "        vel1 = 'y_velocity'\n",
    "        vel2 = 'z_velocity'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction. Choose from 'x', 'y', 'z'.\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df = arrange_slice_df(input_file, direction, cubic=False)\n",
    "\n",
    "    # Extract unique coordinate values, sorted in ascending order\n",
    "    coord1_values = np.sort(df[coord1].unique())\n",
    "    coord2_values = np.sort(df[coord2].unique())\n",
    "\n",
    "    # Calculate the 'dx' between coordinate values for grid step size\n",
    "    d_coord1 = np.diff(coord1_values).mean()\n",
    "    d_coord2 = np.diff(coord2_values).mean()\n",
    "\n",
    "    grid_coord1, grid_coord2 = np.meshgrid(coord1_values, coord2_values, indexing='ij')\n",
    "\n",
    "    # Step 2: Create a structured 3D grid for visualization\n",
    "    # Initialize x, y, z coordinates for the structured grid\n",
    "    if direction == 'z':\n",
    "        grid = pv.StructuredGrid(grid_coord1, grid_coord2, np.zeros_like(grid_coord1))\n",
    "    elif direction == 'y':\n",
    "        grid = pv.StructuredGrid(grid_coord1, np.zeros_like(grid_coord1), grid_coord2)\n",
    "    elif direction == 'x':\n",
    "        grid = pv.StructuredGrid(np.zeros_like(grid_coord1), grid_coord1, grid_coord2)\n",
    "\n",
    "    # Combine velocity components into a single array and assign to the grid\n",
    "    if direction == 'z':\n",
    "        velocity = np.column_stack((df[vel1], df[vel2], np.zeros_like(df[vel1])))\n",
    "    elif direction == 'y':\n",
    "        velocity = np.column_stack((df[vel1], np.zeros_like(df[vel1]), df[vel2]))\n",
    "    elif direction == 'x':\n",
    "        velocity = np.column_stack((np.zeros_like(df[vel1]), df[vel1], df[vel2]))\n",
    "\n",
    "    grid.point_data['velocity'] = velocity\n",
    "\n",
    "    # Step 3: Generate streamlines from seed points (initial points)\n",
    "    # Define seed points across the flow domain\n",
    "    seed_coord1, seed_coord2 = np.meshgrid(\n",
    "        np.linspace(coord1_values[0], coord1_values[-1], seed_points_resolution[0]),\n",
    "        np.linspace(coord2_values[0], coord2_values[-1], seed_points_resolution[1])\n",
    "    )\n",
    "    seed_coord1 = seed_coord1.ravel()\n",
    "    seed_coord2 = seed_coord2.ravel()\n",
    "\n",
    "    if direction == 'z':\n",
    "        x_seed = seed_coord1\n",
    "        y_seed = seed_coord2\n",
    "        z_seed = np.zeros_like(x_seed)\n",
    "    elif direction == 'y':\n",
    "        x_seed = seed_coord1\n",
    "        y_seed = np.zeros_like(seed_coord1)\n",
    "        z_seed = seed_coord2\n",
    "    elif direction == 'x':\n",
    "        x_seed = np.zeros_like(seed_coord1)\n",
    "        y_seed = seed_coord1\n",
    "        z_seed = seed_coord2\n",
    "\n",
    "    # Combine coordinates to form seed points\n",
    "    seed_points = np.column_stack((x_seed, y_seed, z_seed))\n",
    "    seed = pv.PolyData(seed_points)\n",
    "\n",
    "    # Calculate streamlines based on velocity field, starting from the seed points\n",
    "    streamlines = grid.streamlines_from_source(\n",
    "        source=seed,\n",
    "        vectors='velocity',\n",
    "        integration_direction=integration_direction,\n",
    "        max_time=max_time,\n",
    "        initial_step_length=0.5*(d_coord1+d_coord2),\n",
    "        terminal_speed=terminal_speed\n",
    "    )\n",
    "\n",
    "    # Calculate and add velocity magnitude as a scalar field on streamlines for coloring\n",
    "    velocity_vectors = streamlines['velocity']\n",
    "    velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "    streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "    # Step 4: Visualize and export streamlines as HTML\n",
    "    plotter = pv.Plotter(off_screen=True)\n",
    "    plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "    # Color the streamlines by velocity magnitude and set up a scalar bar\n",
    "    plotter.add_mesh(\n",
    "        streamlines.tube(radius=0.5 * (d_coord1+d_coord2) * 0.5),\n",
    "        scalars='velocity_magnitude',\n",
    "        cmap=cmap,  # Use the colormap specified in the function argument\n",
    "        scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    "    )\n",
    "    if direction == 'z':\n",
    "        # Set the view based on the specified direction\n",
    "        plotter.view_xy()\n",
    "    elif direction == 'y':\n",
    "        plotter.view_xz()\n",
    "    elif direction == 'x':\n",
    "        plotter.view_yz()\n",
    "\n",
    "    # Show grid with axis labels\n",
    "    plotter.show_grid(\n",
    "        xtitle='X',\n",
    "        ytitle='Y',\n",
    "        ztitle='Z',\n",
    "        grid='front'  # Display the grid in front of the scene\n",
    "    )\n",
    "\n",
    "    # Adjust axis limits by adding an invisible box\n",
    "    if axis_limits is not None:\n",
    "        var1_min, var1_max, var2_min, var2_max = axis_limits\n",
    "\n",
    "        if direction == 'z':\n",
    "            # For 'z' direction, create a box with x and y limits\n",
    "            box = pv.Box(bounds=(\n",
    "                var1_min, var1_max,  # x bounds\n",
    "                var2_min, var2_max,  # y bounds\n",
    "                0, 0                # z bounds (since it's a 2D plane at z=0)\n",
    "            ))\n",
    "        elif direction == 'y':\n",
    "            # For 'y' direction, create a box with x and z limits\n",
    "            box = pv.Box(bounds=(\n",
    "                var1_min, var1_max,  # x bounds\n",
    "                0, 0,               # y bounds (since it's a 2D plane at y=0)\n",
    "                var2_min, var2_max   # z bounds\n",
    "            ))\n",
    "        elif direction == 'x':\n",
    "            # For 'x' direction, create a box with y and z limits\n",
    "            box = pv.Box(bounds=(\n",
    "                0, 0,               # x bounds (since it's a 2D plane at x=0)\n",
    "                var1_min, var1_max,  # y bounds\n",
    "                var2_min, var2_max   # z bounds\n",
    "            ))\n",
    "\n",
    "        # Add the box to the plotter with zero opacity\n",
    "        plotter.add_mesh(box, opacity=0.0, show_edges=False)\n",
    "\n",
    "    plotter.export_html(output_file)\n",
    "\n",
    "plot_3D_to_2D_slice_streamline(\"3D_DATA/ra10e7_result/ra10e7_100.csv\", \"streamline.html\", 'y', [20,20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
