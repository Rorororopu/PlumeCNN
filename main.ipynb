{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import analyzer\n",
    "import neural_network as nn\n",
    "import convolutional_neural_network as cnn\n",
    "import visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = [250,250]\n",
    "# Resolution should be consistent throughout the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Because the training dataset are individual, the list of each data just contain themselves.\n",
    "data1 = analyzer.Data(\"MORE_DATA/db_Y_0027.okc\", resolution) \n",
    "data2 = analyzer.Data(\"MORE_DATA/db_Y_0030.okc\", resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use NN model\n",
    "\n",
    "array1, headers1, non_nan_indices1, num_grids1 = nn.data_arranger(data1.df)\n",
    "array2, headers2, non_nan_indices2, num_grids2 = nn.data_arranger(data2.df)\n",
    "\n",
    "# The learning rate, batch size, and epochs are proven to be working.\n",
    "\n",
    "nn_model = nn.model_create_compile(headers1, 0.05)\n",
    "\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array1, 1000, 10)\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array2, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use CNN model\n",
    "\n",
    "array1, headers1, indices1 = cnn.data_arranger(data1.df, resolution)\n",
    "array2, headers2, indices2 = cnn.data_arranger(data2.df, resolution)\n",
    "\n",
    "# The learning rate and epochs are proven to be working.\n",
    "\n",
    "cnn_model = cnn.model_2D_create_compile(headers1, 0.05, resolution)\n",
    "\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array1, 3)\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = analyzer.Data(\"MORE_DATA/db_Y_0049.okc\", resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "array3, headers3, non_nan_indices3, num_grids3 = nn.data_arranger(data3.df)\n",
    "data3.df = nn.model_classification(nn_model, array3, non_nan_indices3, num_grids3, data3.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "array3, headers3, indices3 = cnn.data_arranger(data3.df, resolution)\n",
    "data3.df = cnn.model_2D_classification(cnn_model, array3, indices3, data3.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_classify = [f\"MORE_DATA/db_Y_{i:04d}.okc\" for i in range(99)]\n",
    "\n",
    "Data_classify = [analyzer.Data(path, list_paths_classify, resolution) for path in list_paths_classify] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "for data in Data_classify:\n",
    "    array, headers, non_nan_indices, num_grids = nn.data_arranger(data.df)\n",
    "    data.df = nn.model_classification(nn_model, array, non_nan_indices, num_grids, data.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "for data in Data_classify:\n",
    "    array, headers, indices = cnn.data_arranger(data.df, data.resolution)\n",
    "    data.df = cnn.model_2D_classification(cnn_model, array, indices, data.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "visualizer.plot_2D_df(data3.df, 'is_boundary', 'classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "data3.df.to_csv('classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 3D CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "path = '3D_DATA/small_db_result'\n",
    "output_file = '3D_DATA/small_db.csv'\n",
    "\n",
    "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = None\n",
    "    \n",
    "    for index, filename in enumerate(files):\n",
    "        print(f\"merging file: {index + 1}/{len(files)}\", flush=True)\n",
    "        \n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            header = next(reader)\n",
    "            \n",
    "            if writer is None:\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"All files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import plot_3D_to_2D_slice_streamline\n",
    "\n",
    "plot_3D_to_2D_slice_streamline(input_file=\"3D_DATA/ra10e7_result/ra10e7_100.csv\", output_file=\"streamlines.html\", direction='y', seed_points_resolution=[20,20], max_time=0.2, cmap = 'viridis', axis_limits=[-0.5,0.5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ from a manually made velocity field, plot streamline (for test)\n",
    "\n",
    "✅ generate a csv file from a set velocity field(for test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n",
    "\n",
    "# Extract unique coordinate values for each axis (ensure they are sorted)\n",
    "x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())\n",
    "\n",
    "# Determine grid dimensions\n",
    "nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the StructuredGrid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Add the velocity vectors\n",
    "grid.point_data['velocity'] = velocities\n",
    "\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(-0.5, 0.5, 3), # min, max, num\n",
    "    np.linspace(-0.5, 0.5, 3),\n",
    "    np.linspace(-0.5, 0.5, 3)\n",
    "    )\n",
    "seed_x = seed_x.ravel()\n",
    "seed_y = seed_y.ravel()\n",
    "seed_z = seed_z.ravel()\n",
    "\n",
    "seed_points = np.column_stack((seed_x, seed_y, seed_z))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")\n",
    "\n",
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.01),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# in such/a/folder/name_result, the CSVs will be like name_1.csv, name_2.csv, ...\n",
    "\n",
    "# Read and concatenate all CSV files in the folder\n",
    "folder_path = '3D_DATA/ra10e7_result'\n",
    "# Extract the last part of the path and remove 'result'\n",
    "base_name = os.path.basename(folder_path).replace('result', '')\n",
    "# Create the search pattern\n",
    "pattern = f\"{base_name}*.csv\"\n",
    "# Get sorted list of matching CSV files\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, pattern)))\n",
    "df_list = [pd.read_csv(f) for f in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Extract coordinates and velocity vectors\n",
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted unique coordinate values along each axis\n",
    "x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())\n",
    "\n",
    "# Grid dimensions\n",
    "nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates to match structured grid shape\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the structured grid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Attach velocity vectors as point data\n",
    "grid.point_data['velocity'] = velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(-0.5, 0.5, 3), # min, max, num\n",
    "    np.linspace(-0.5, 0.5, 3),\n",
    "    np.linspace(-0.5, 0.5, 3)\n",
    "    )\n",
    "seed_x = seed_x.ravel()\n",
    "seed_y = seed_y.ravel()\n",
    "seed_z = seed_z.ravel()\n",
    "\n",
    "seed_points = np.column_stack((seed_x, seed_y, seed_z))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")\n",
    "\n",
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.01),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
