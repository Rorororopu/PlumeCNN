{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import analyzer\n",
    "import neural_network as nn\n",
    "import convolutional_neural_network as cnn\n",
    "import visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = [250,250]\n",
    "# Resolution should be consistent throughout the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Because the training dataset are individual, the list of each data just contain themselves.\n",
    "data1 = analyzer.Data(\"MORE_DATA/db_Y_0027.okc\", resolution) \n",
    "data2 = analyzer.Data(\"MORE_DATA/db_Y_0030.okc\", resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use NN model\n",
    "\n",
    "array1, headers1, non_nan_indices1, num_grids1 = nn.data_arranger(data1.df)\n",
    "array2, headers2, non_nan_indices2, num_grids2 = nn.data_arranger(data2.df)\n",
    "\n",
    "# The learning rate, batch size, and epochs are proven to be working.\n",
    "\n",
    "nn_model = nn.model_create_compile(headers1, 0.05)\n",
    "\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array1, 1000, 10)\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array2, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use CNN model\n",
    "\n",
    "array1, headers1, indices1 = cnn.data_arranger(data1.df, resolution)\n",
    "array2, headers2, indices2 = cnn.data_arranger(data2.df, resolution)\n",
    "\n",
    "# The learning rate and epochs are proven to be working.\n",
    "\n",
    "cnn_model = cnn.model_2D_create_compile(headers1, 0.05, resolution)\n",
    "\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array1, 3)\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = analyzer.Data(\"MORE_DATA/db_Y_0049.okc\", resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "array3, headers3, non_nan_indices3, num_grids3 = nn.data_arranger(data3.df)\n",
    "data3.df = nn.model_classification(nn_model, array3, non_nan_indices3, num_grids3, data3.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "array3, headers3, indices3 = cnn.data_arranger(data3.df, resolution)\n",
    "data3.df = cnn.model_2D_classification(cnn_model, array3, indices3, data3.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_classify = [f\"MORE_DATA/db_Y_{i:04d}.okc\" for i in range(99)]\n",
    "\n",
    "Data_classify = [analyzer.Data(path, list_paths_classify, resolution) for path in list_paths_classify] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "for data in Data_classify:\n",
    "    array, headers, non_nan_indices, num_grids = nn.data_arranger(data.df)\n",
    "    data.df = nn.model_classification(nn_model, array, non_nan_indices, num_grids, data.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "for data in Data_classify:\n",
    "    array, headers, indices = cnn.data_arranger(data.df, data.resolution)\n",
    "    data.df = cnn.model_2D_classification(cnn_model, array, indices, data.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "visualizer.plot_2D_df(data3.df, 'is_boundary', 'classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "data3.df.to_csv('classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 3D CSVs\n",
    "\n",
    "will remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "path = '3D_DATA/small_db_result'\n",
    "output_file = '3D_DATA/small_db.csv'\n",
    "\n",
    "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = None\n",
    "    \n",
    "    for index, filename in enumerate(files):\n",
    "        print(f\"merging file: {index + 1}/{len(files)}\", flush=True)\n",
    "        \n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            header = next(reader)\n",
    "            \n",
    "            if writer is None:\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"All files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import plot_3D_to_2D_slice_streamline\n",
    "\n",
    "plot_3D_to_2D_slice_streamline(input_file=\"3D_DATA/ra10e7_result/ra10e7_100.csv\", output_file=\"streamlines.html\", direction='y', seed_points_resolution=[20,20], max_time=0.2, cmap = 'viridis', axis_limits=[-0.5,0.5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ from a manually made velocity field, plot streamline (for test)\n",
    "\n",
    "✅ generate a csv file from a set velocity field(for test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n",
    "\n",
    "# Extract unique coordinate values for each axis (ensure they are sorted)\n",
    "x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())\n",
    "\n",
    "# Determine grid dimensions\n",
    "nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the StructuredGrid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Add the velocity vectors\n",
    "grid.point_data['velocity'] = velocities\n",
    "\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(-0.5, 0.5, 3), # min, max, num\n",
    "    np.linspace(-0.5, 0.5, 3),\n",
    "    np.linspace(-0.5, 0.5, 3)\n",
    "    )\n",
    "seed_x = seed_x.ravel()\n",
    "seed_y = seed_y.ravel()\n",
    "seed_z = seed_z.ravel()\n",
    "\n",
    "seed_points = np.column_stack((seed_x, seed_y, seed_z))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")\n",
    "\n",
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.01),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dealing with empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_empty_rows(file_path:str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fills in the empty rows in a CSV file that represents a Y-slice 2D data grid. \n",
    "    The data is structured in repeating \"sandwich\" blocks where each block contains:\n",
    "        [empty rows] + [non-empty data rows] + [empty rows].\n",
    "    The missing rows (empty) occur due to a truncated `x` range. Non-empty data rows have same z coordinate\n",
    "    but different x coordinates. The dataset is already on a regular grid, so no interpolation is needed. \n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the input CSV file containing possibly incomplete 2D slice data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            A DataFrame with the same shape as the original but with all empty rows filled in. \n",
    "            Only includes the essential columns: \n",
    "            ['x', 'y', 'z', 'x_velocity', 'y_velocity', 'z_velocity'].\n",
    "    '''\n",
    "\n",
    "    # Step 1: read df(dataframe)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    required_columns = ['x', 'y', 'z', 'x_velocity', 'y_velocity', 'z_velocity']\n",
    "    df = df[required_columns]\n",
    "\n",
    "    df_filled = df.copy()\n",
    "    empty_mask = df.isna().all(axis=1) # A list length is num of rows. If all row is empty, corresponding element is True.\n",
    "\n",
    "    # Step 2: Count initial empty rows\n",
    "    first_non_empty_idx = empty_mask.idxmin()\n",
    "    num_empty_rows = empty_mask[:first_non_empty_idx].sum()\n",
    "\n",
    "    # Step 3: Count non-empty rows in the first pack\n",
    "    idx = first_non_empty_idx\n",
    "    while not empty_mask.iloc[idx]: # iloc[idx]: if that index is True, then True\n",
    "        idx += 1\n",
    "    num_non_empty_rows = idx - first_non_empty_idx\n",
    "\n",
    "    # Step 5: Get x_diff from first block of non-empty rows\n",
    "    x_vals = df_filled.loc[first_non_empty_idx:idx - 1, 'x'].values #loc: range of row, header, values() is return np array\n",
    "    x_diff = np.diff(x_vals).mean()\n",
    "\n",
    "    # Step 6: Precompute total number of \"sandwiches\"\n",
    "    group_size = num_empty_rows * 2 + num_non_empty_rows\n",
    "    total_groups = len(df_filled) // group_size\n",
    "\n",
    "    for i in range(total_groups):\n",
    "        # Calculate indices for each part of the sandwich\n",
    "        start_idx = i * group_size\n",
    "        mid_idx = start_idx + num_empty_rows\n",
    "        end_idx = mid_idx + num_non_empty_rows\n",
    "\n",
    "        # First and last x values of the current non-empty block\n",
    "        block = df_filled.iloc[mid_idx:end_idx]\n",
    "        if block['x'].isna().all():\n",
    "            continue  # This block has no valid data, skip\n",
    "\n",
    "        x_first = block['x'].iloc[0]\n",
    "        x_last = block['x'].iloc[-1]\n",
    "\n",
    "        y_first = block['y'].iloc[0]\n",
    "        z_first = block['z'].iloc[0]\n",
    "        y_last = block['y'].iloc[-1]\n",
    "        z_last = block['z'].iloc[-1]\n",
    "\n",
    "        # Build x values for rows ABOVE the block\n",
    "        x_above = x_first - np.arange(num_empty_rows, 0, -1) * x_diff\n",
    "        rows_above = pd.DataFrame({\n",
    "            'x': x_above,\n",
    "            'y': y_first,\n",
    "            'z': z_first,\n",
    "            'x_velocity': 0.0,\n",
    "            'y_velocity': 0.0,\n",
    "            'z_velocity': 0.0\n",
    "        })\n",
    "\n",
    "        # Build x values for rows BELOW the block\n",
    "        x_below = x_last + np.arange(1, num_empty_rows + 1) * x_diff\n",
    "        rows_below = pd.DataFrame({\n",
    "            'x': x_below,\n",
    "            'y': y_last,\n",
    "            'z': z_last,\n",
    "            'x_velocity': 0.0,\n",
    "            'y_velocity': 0.0,\n",
    "            'z_velocity': 0.0\n",
    "        })\n",
    "\n",
    "        # Insert filled data into the copied DataFrame\n",
    "        df_filled.iloc[start_idx:mid_idx] = rows_above.values\n",
    "        df_filled.iloc[end_idx:end_idx + num_empty_rows] = rows_below.values\n",
    "\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import glob\n",
    "import os\n",
    "# in such/a/folder/name_result, the CSVs will be like name_1.csv, name_2.csv, ...\n",
    "\n",
    "# Read and concatenate all CSV files in the folder\n",
    "folder_path = '3D_DATA/ra10e7_result'\n",
    "# Extract the last part of the path and remove 'result'\n",
    "base_name = os.path.basename(folder_path).replace('result', '')\n",
    "# Create the search pattern\n",
    "pattern = f\"{base_name}*.csv\"\n",
    "# Get sorted list of matching CSV files\n",
    "csv_files = glob.glob(os.path.join(folder_path, pattern)) # don't use sort\n",
    "\n",
    "df_list = [fill_empty_rows(f) for f in csv_files]\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = df[['x', 'y', 'z']].values\n",
    "velocities = df[['x_velocity', 'y_velocity', 'z_velocity']].values\n",
    "\n",
    "# Extract unique coordinate values for each axis (ensure they are sorted)\n",
    "'''x_vals = np.sort(df['x'].unique())\n",
    "y_vals = np.sort(df['y'].unique())\n",
    "z_vals = np.sort(df['z'].unique())'''\n",
    "\n",
    "# Determine grid dimensions\n",
    "nx = 200\n",
    "ny = 200\n",
    "nz = 198\n",
    "# nx, ny, nz = len(x_vals), len(y_vals), len(z_vals)\n",
    "\n",
    "# Reshape coordinates\n",
    "x = df['x'].values.reshape((nx, ny, nz))\n",
    "y = df['y'].values.reshape((nx, ny, nz))\n",
    "z = df['z'].values.reshape((nx, ny, nz))\n",
    "\n",
    "# Create the StructuredGrid\n",
    "grid = pv.StructuredGrid(x, y, z)\n",
    "\n",
    "# Add the velocity vectors\n",
    "grid.point_data['velocity'] = velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, ymin, ymax, zmin, zmax = grid.bounds\n",
    "\n",
    "# Create a 5×5×5 grid of seed points across the domain\n",
    "seed_x, seed_y, seed_z = np.meshgrid(\n",
    "    np.linspace(xmin, xmax, 10),\n",
    "    np.linspace(ymin, ymax, 10),\n",
    "    np.linspace(zmin, zmax, 10),\n",
    "    indexing='ij'\n",
    ")\n",
    "\n",
    "seed_points = np.column_stack((\n",
    "    seed_x.ravel(), seed_y.ravel(), seed_z.ravel()\n",
    "))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='forward',\n",
    "    max_time=0.1,\n",
    "    initial_step_length=0.01,\n",
    "    terminal_speed=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "# Visualize and export streamlines as HTML\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.005),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "\n",
    "plotter.view_isometric()\n",
    "# Show grid with axis labels\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "plotter.export_html('output_file.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[40000 * 99 : 40000 * 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-0.5, 0.5, 200)\n",
    "z_values = np.linspace(0, 1, 200)\n",
    "\n",
    "dx = np.diff(x_values).mean()\n",
    "dz = np.diff(z_values).mean()\n",
    "\n",
    "grid_x, grid_z = np.meshgrid(x_values, z_values, indexing='ij')\n",
    "grid = pv.StructuredGrid(grid_x, np.zeros_like(grid_x), grid_z)\n",
    "velocity = np.column_stack((df_test['x_velocity'], np.zeros_like(df_test['x_velocity']), df_test['z_velocity']))\n",
    "grid.point_data['velocity'] = velocity\n",
    "\n",
    "x_seed, z_seed = np.meshgrid(\n",
    "        np.linspace(x_values[0], x_values[-1], 10),\n",
    "        np.linspace(z_values[0], z_values[-1], 10)\n",
    "    )\n",
    "x_seed = x_seed.ravel()\n",
    "z_seed = z_seed.ravel()\n",
    "y_seed = np.zeros_like(x_seed)\n",
    "\n",
    "seed_points = np.column_stack((x_seed, y_seed, z_seed))\n",
    "seed = pv.PolyData(seed_points)\n",
    "\n",
    "streamlines = grid.streamlines_from_source(\n",
    "    source=seed,\n",
    "    vectors='velocity',\n",
    "    integration_direction='both',\n",
    "    max_time=10,\n",
    "    initial_step_length=0.5*(dx+dz),\n",
    "    terminal_speed=1e-3\n",
    ")\n",
    "\n",
    "velocity_vectors = streamlines['velocity']\n",
    "velocity_magnitude = np.linalg.norm(velocity_vectors, axis=1)\n",
    "streamlines['velocity_magnitude'] = velocity_magnitude\n",
    "\n",
    "plotter = pv.Plotter(off_screen=True)\n",
    "plotter.add_mesh(grid.outline(), color='k')\n",
    "\n",
    "plotter.add_mesh(\n",
    "    streamlines.tube(radius=0.5 * (dx+dz) * 0.5),\n",
    "    scalars='velocity_magnitude',\n",
    "    cmap='viridis',  # Use the colormap specified in the function argument\n",
    "    scalar_bar_args={'title': 'Velocity Magnitude'}\n",
    ")\n",
    "plotter.view_xz()\n",
    "\n",
    "\n",
    "plotter.show_grid(\n",
    "    xtitle='X',\n",
    "    ytitle='Y',\n",
    "    ztitle='Z',\n",
    "    grid='front'  # Display the grid in front of the scene\n",
    ")\n",
    "\n",
    "box = pv.Box(bounds=(\n",
    "    -0.5, 0.5,  # x bounds\n",
    "    0, 0,   # y bounds (since it's a 2D plane at y=0)\n",
    "    0, 1   # z bounds\n",
    "))\n",
    "\n",
    "plotter.add_mesh(box, opacity=0.0, show_edges=False)\n",
    "\n",
    "plotter.export_html(\"streamlines.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
