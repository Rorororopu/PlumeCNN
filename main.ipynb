{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import analyzer\n",
    "import neural_network as nn\n",
    "import convolutional_neural_network as cnn\n",
    "import visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = [250,250]\n",
    "# Resolution should be consistent throughout the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Because the training dataset are individual, the list of each data just contain themselves.\n",
    "data1 = analyzer.Data(\"MORE_DATA/db_Y_0027.okc\", resolution) \n",
    "data2 = analyzer.Data(\"MORE_DATA/db_Y_0030.okc\", resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use NN model\n",
    "\n",
    "array1, headers1, non_nan_indices1, num_grids1 = nn.data_arranger(data1.df)\n",
    "array2, headers2, non_nan_indices2, num_grids2 = nn.data_arranger(data2.df)\n",
    "\n",
    "# The learning rate, batch size, and epochs are proven to be working.\n",
    "\n",
    "nn_model = nn.model_create_compile(headers1, 0.05)\n",
    "\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array1, 1000, 10)\n",
    "nn_model, loss_hist = nn.model_train(nn_model, array2, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section if you want to use CNN model\n",
    "\n",
    "array1, headers1, indices1 = cnn.data_arranger(data1.df, resolution)\n",
    "array2, headers2, indices2 = cnn.data_arranger(data2.df, resolution)\n",
    "\n",
    "# The learning rate and epochs are proven to be working.\n",
    "\n",
    "cnn_model = cnn.model_2D_create_compile(headers1, 0.05, resolution)\n",
    "\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array1, 3)\n",
    "cnn_model, loss_hist = cnn.model_2D_train(cnn_model, array2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = analyzer.Data(\"MORE_DATA/db_Y_0049.okc\", resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "array3, headers3, non_nan_indices3, num_grids3 = nn.data_arranger(data3.df)\n",
    "data3.df = nn.model_classification(nn_model, array3, non_nan_indices3, num_grids3, data3.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "array3, headers3, indices3 = cnn.data_arranger(data3.df, resolution)\n",
    "data3.df = cnn.model_2D_classification(cnn_model, array3, indices3, data3.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 3 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths_classify = [f\"MORE_DATA/db_Y_{i:04d}.okc\" for i in range(99)]\n",
    "\n",
    "Data_classify = [analyzer.Data(path, list_paths_classify, resolution) for path in list_paths_classify] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is NN\n",
    "for data in Data_classify:\n",
    "    array, headers, non_nan_indices, num_grids = nn.data_arranger(data.df)\n",
    "    data.df = nn.model_classification(nn_model, array, non_nan_indices, num_grids, data.df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code if your model choice is CNN\n",
    "for data in Data_classify:\n",
    "    array, headers, indices = cnn.data_arranger(data.df, data.resolution)\n",
    "    data.df = cnn.model_2D_classification(cnn_model, array, indices, data.df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify few individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "visualizer.plot_2D_df(data3.df, 'is_boundary', 'classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "data3.df.to_csv('classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose codes from these 2 codes below to run if you want to classify a series of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the image\n",
    "for i, data in enumerate(Data_classify):\n",
    "    visualizer.plot_2D_df(data.df, 'is_boundary', f'classification_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to export the csv file\n",
    "for i, data in enumerate(Data_classify):\n",
    "    data.df.to_csv(f'classification_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# 指定文件夹路径\n",
    "path = '3D_DATA/small_db_result'\n",
    "output_file = '3D_DATA/small_db.csv'\n",
    "\n",
    "# 获取所有csv文件的文件名\n",
    "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "# 打开输出文件，准备写入合并后的内容\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = None  # 创建一个CSV写入器，但先不初始化\n",
    "    \n",
    "    # 遍历所有CSV文件\n",
    "    for index, filename in enumerate(files):\n",
    "        # 显示进度\n",
    "        print(f\"merging file: {index + 1}/{len(files)}\", flush=True)\n",
    "        \n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            # 获取文件的第一行（标题）\n",
    "            header = next(reader)\n",
    "            \n",
    "            # 如果是第一个文件，写入标题\n",
    "            if writer is None:\n",
    "                writer = csv.writer(outfile)\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            # 从第二行开始读取并写入\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"All files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import plot_3D_to_2D_slice_streamline\n",
    "plot_3D_to_2D_slice_streamline(input_file=\"3D_DATA/ra10e7_result/ra10e7_100.csv\", output_file=\"streamlines.html\", direction='y', seed_points_resolution=[20,20], max_time=0.2, cmap = 'viridis', axis_limits=[-0.5,0.5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Streamline 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Plotter' object has no attribute 'set_xlim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisualizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_3D_streamline\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_3D_streamline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3D_DATA/small_db.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstreamlines.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_points_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegration_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_limits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/fs001/home/wtian/PlumeCNN/visualizer.py:502\u001b[0m, in \u001b[0;36mplot_3D_streamline\u001b[0;34m(input_file, output_file, seed_points_resolution, integration_direction, max_time, terminal_speed, cmap, axis_limits)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     x_min, x_max, y_min, y_max, z_min, z_max \u001b[38;5;241m=\u001b[39m axis_limits\n\u001b[0;32m--> 502\u001b[0m     \u001b[43mplotter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_xlim\u001b[49m(x_min, x_max)\n\u001b[1;32m    503\u001b[0m     plotter\u001b[38;5;241m.\u001b[39mset_ylim(y_min, y_max)\n\u001b[1;32m    504\u001b[0m     plotter\u001b[38;5;241m.\u001b[39mset_zlim(z_min, z_max)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Plotter' object has no attribute 'set_xlim'"
     ]
    }
   ],
   "source": [
    "from visualizer import plot_3D_streamline\n",
    "plot_3D_streamline(input_file=\"3D_DATA/small_db.csv\", output_file=\"streamlines.html\", seed_points_resolution=[4,4,4], integration_direction='forward', max_time=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
